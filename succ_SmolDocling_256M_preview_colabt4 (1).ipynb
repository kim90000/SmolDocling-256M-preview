{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVpsCDEWHd13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCYC5SgbTjSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Prerequisites:\n",
        "# pip install onnxruntime\n",
        "# pip install onnxruntime-gpu"
      ],
      "metadata": {
        "id": "MewUjD6dTjWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "YM9PL0-GTtwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-gpu"
      ],
      "metadata": {
        "id": "majGQMsATwDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docling_core"
      ],
      "metadata": {
        "id": "n9zzkFYSUriQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoConfig, AutoProcessor\n",
        "from transformers.image_utils import load_image\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "import os\n",
        "from docling_core.types.doc import DoclingDocument\n",
        "from docling_core.types.doc.document import DocTagsDocument\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "# cuda\n",
        "os.environ[\"ORT_CUDA_USE_MAX_WORKSPACE\"] = \"1\"\n",
        "\n",
        "# 1. Load models\n",
        "## Load config and processor\n",
        "model_id = \"ds4sd/SmolDocling-256M-preview\"\n",
        "config = AutoConfig.from_pretrained(model_id)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "## Load sessions\n",
        "# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/vision_encoder.onnx\n",
        "# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/embed_tokens.onnx\n",
        "# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/decoder_model_merged.onnx\n",
        "# cpu\n",
        "# vision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\")\n",
        "# embed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\")\n",
        "# decoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\"\n",
        "\n",
        "# cuda\n",
        "vision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "embed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "decoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "\n",
        "## Set config values\n",
        "num_key_value_heads = config.text_config.num_key_value_heads\n",
        "head_dim = config.text_config.head_dim\n",
        "num_hidden_layers = config.text_config.num_hidden_layers\n",
        "eos_token_id = config.text_config.eos_token_id\n",
        "image_token_id = config.image_token_id\n",
        "end_of_utterance_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_utterance>\")\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "## Load image and apply processor\n",
        "image = load_image(\"https://ibm.biz/docling-page-with-table\")\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image], return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "image_features = None\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "position_ids = np.cumsum(inputs['attention_mask'], axis=-1)\n",
        "\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 8192\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  inputs_embeds = embed_session.run(None, {'input_ids': input_ids})[0]\n",
        "\n",
        "  if image_features is None:\n",
        "    ## Only compute vision features if not already computed\n",
        "    image_features = vision_session.run(\n",
        "        ['image_features'],  # List of output names or indices\n",
        "        {\n",
        "            'pixel_values': inputs['pixel_values'],\n",
        "            'pixel_attention_mask': inputs['pixel_attention_mask'].astype(np.bool_)\n",
        "        }\n",
        "    )[0]\n",
        "\n",
        "    ## Merge text and vision embeddings\n",
        "    inputs_embeds[inputs['input_ids'] == image_token_id] = image_features.reshape(-1, image_features.shape[-1])\n",
        "\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      inputs_embeds=inputs_embeds,\n",
        "      attention_mask=attention_mask,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  attention_mask = np.ones_like(input_ids)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all() or (input_ids == end_of_utterance_id).all():\n",
        "    break  # Stop predicting\n",
        "\n",
        "doctags = processor.batch_decode(\n",
        "    generated_tokens,\n",
        "    skip_special_tokens=False,\n",
        ")[0].lstrip()\n",
        "\n",
        "print(doctags)\n",
        "\n",
        "doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n",
        "print(doctags)\n",
        "# create a docling document\n",
        "doc = DoclingDocument(name=\"Document\")\n",
        "doc.load_from_doctags(doctags_doc)\n",
        "\n",
        "print(doc.export_to_markdown())"
      ],
      "metadata": {
        "id": "ctFleYrzTjYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoConfig, AutoProcessor\n",
        "from transformers.image_utils import load_image\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "import os\n",
        "from docling_core.types.doc import DoclingDocument\n",
        "from docling_core.types.doc.document import DocTagsDocument\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "# cuda\n",
        "os.environ[\"ORT_CUDA_USE_MAX_WORKSPACE\"] = \"1\"\n",
        "\n",
        "# 1. Load models\n",
        "## Load config and processor\n",
        "model_id = \"ds4sd/SmolDocling-256M-preview\"\n",
        "config = AutoConfig.from_pretrained(model_id)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "## Load sessions\n",
        "# Download the ONNX models if they are not present\n",
        "!wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/vision_encoder.onnx\n",
        "!wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/embed_tokens.onnx\n",
        "!wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/decoder_model_merged.onnx\n",
        "# cpu\n",
        "# vision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\")\n",
        "# embed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\")\n",
        "# decoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\"\n",
        "\n",
        "# cuda\n",
        "vision_session = onnxruntime.InferenceSession(\"vision_encoder.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "embed_session = onnxruntime.InferenceSession(\"embed_tokens.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "decoder_session = onnxruntime.InferenceSession(\"decoder_model_merged.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "\n",
        "# ... rest of the code ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "j01PDEpGXTmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Set config values\n",
        "num_key_value_heads = config.text_config.num_key_value_heads\n",
        "head_dim = config.text_config.head_dim\n",
        "num_hidden_layers = config.text_config.num_hidden_layers\n",
        "eos_token_id = config.text_config.eos_token_id\n",
        "image_token_id = config.image_token_id\n",
        "end_of_utterance_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_utterance>\")\n",
        "\n",
        "# 2. Prepare inputs\n",
        "## Create input messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "## Load image and apply processor\n",
        "image = load_image(\"https://ibm.biz/docling-page-with-table\")\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image], return_tensors=\"np\")\n",
        "\n",
        "## Prepare decoder inputs\n",
        "batch_size = inputs['input_ids'].shape[0]\n",
        "past_key_values = {\n",
        "    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\n",
        "    for layer in range(num_hidden_layers)\n",
        "    for kv in ('key', 'value')\n",
        "}\n",
        "image_features = None\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']\n",
        "position_ids = np.cumsum(inputs['attention_mask'], axis=-1)\n",
        "\n",
        "\n",
        "# 3. Generation loop\n",
        "max_new_tokens = 8192\n",
        "generated_tokens = np.array([[]], dtype=np.int64)\n",
        "for i in range(max_new_tokens):\n",
        "  inputs_embeds = embed_session.run(None, {'input_ids': input_ids})[0]\n",
        "\n",
        "  if image_features is None:\n",
        "    ## Only compute vision features if not already computed\n",
        "    image_features = vision_session.run(\n",
        "        ['image_features'],  # List of output names or indices\n",
        "        {\n",
        "            'pixel_values': inputs['pixel_values'],\n",
        "            'pixel_attention_mask': inputs['pixel_attention_mask'].astype(np.bool_)\n",
        "        }\n",
        "    )[0]\n",
        "\n",
        "    ## Merge text and vision embeddings\n",
        "    inputs_embeds[inputs['input_ids'] == image_token_id] = image_features.reshape(-1, image_features.shape[-1])\n",
        "\n",
        "  logits, *present_key_values = decoder_session.run(None, dict(\n",
        "      inputs_embeds=inputs_embeds,\n",
        "      attention_mask=attention_mask,\n",
        "      position_ids=position_ids,\n",
        "      **past_key_values,\n",
        "  ))\n",
        "\n",
        "  ## Update values for next generation loop\n",
        "  input_ids = logits[:, -1].argmax(-1, keepdims=True)\n",
        "  attention_mask = np.ones_like(input_ids)\n",
        "  position_ids = position_ids[:, -1:] + 1\n",
        "  for j, key in enumerate(past_key_values):\n",
        "    past_key_values[key] = present_key_values[j]\n",
        "\n",
        "  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\n",
        "  if (input_ids == eos_token_id).all() or (input_ids == end_of_utterance_id).all():\n",
        "    break  # Stop predicting\n",
        "\n",
        "doctags = processor.batch_decode(\n",
        "    generated_tokens,\n",
        "    skip_special_tokens=False,\n",
        ")[0].lstrip()\n",
        "\n",
        "print(doctags)\n",
        "\n",
        "doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n",
        "print(doctags)\n",
        "# create a docling document\n",
        "doc = DoclingDocument(name=\"Document\")\n",
        "doc.load_from_doctags(doctags_doc)\n",
        "\n",
        "print(doc.export_to_markdown())"
      ],
      "metadata": {
        "id": "rzK5_0ItXkJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXG8UowwXmpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# مسار الصورة\n",
        "image_path = \"/content/page_with_table.png\"\n",
        "\n",
        "# قراءة الصورة\n",
        "img = mpimg.imread(image_path)\n",
        "\n",
        "# عرض الصورة\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "l06OjCqQYsND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkvGEfXEXmmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HWxMKlopXmjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-NHT1zoXmfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hn1vnyA0XmcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFZ3tc11XmYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZJtxvGcyXmUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prerequisites:\n",
        "# pip install torch\n",
        "# pip install docling_core\n",
        "# pip install transformers\n",
        "\n",
        "import torch\n",
        "from docling_core.types.doc import DoclingDocument\n",
        "from docling_core.types.doc.document import DocTagsDocument\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from transformers.image_utils import load_image\n",
        "from pathlib import Path\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load images\n",
        "image = load_image(\"https://upload.wikimedia.org/wikipedia/commons/7/76/GazettedeFrance.jpg\")\n",
        "\n",
        "# Initialize processor and model\n",
        "processor = AutoProcessor.from_pretrained(\"ds4sd/SmolDocling-256M-preview\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    \"ds4sd/SmolDocling-256M-preview\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n",
        ").to(DEVICE)\n",
        "\n",
        "# Create input messages\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"Convert this page to docling.\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prepare inputs\n",
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
        "inputs = inputs.to(DEVICE)\n",
        "\n",
        "# Generate outputs\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=8192)\n",
        "prompt_length = inputs.input_ids.shape[1]\n",
        "trimmed_generated_ids = generated_ids[:, prompt_length:]\n",
        "doctags = processor.batch_decode(\n",
        "    trimmed_generated_ids,\n",
        "    skip_special_tokens=False,\n",
        ")[0].lstrip()\n",
        "\n",
        "# Populate document\n",
        "doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n",
        "print(doctags)\n",
        "# create a docling document\n",
        "doc = DoclingDocument(name=\"Document\")\n",
        "doc.load_from_doctags(doctags_doc)\n",
        "\n",
        "# export as any format\n",
        "# HTML\n",
        "# output_path_html = Path(\"Out/\") / \"example.html\"\n",
        "# doc.save_as_html(output_filoutput_path_htmle_path)\n",
        "# MD\n",
        "print(doc.export_to_markdown())\n"
      ],
      "metadata": {
        "id": "hiDOOxoEXRbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}